<!DOCTYPE html>
<html lang="en-US">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>EM-LLM: Human-like Episodic Memory for Infinite Context LLMs</title>
    <meta name="description" content="A novel approach integrating human-like episodic memory into Large Language Models for enhanced long-context processing">
    <link rel="stylesheet" href="style.css">
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>EM-LLM: Human-like Episodic Memory for Infinite Context LLMs</h1>
            <p class="authors"><a href="https://zfountas.com/"  class="author-link">Zafeirios Fountas</a>, 
                Martin A Benfeghoul, 
                Adnan Oomerjee, 
                <a href="https://fenchri.github.io/" class="author-link">Fenia Christopoulou</a>, 
                <a href="https://glampouras.github.io/" class="author-link">Gerasimos Lampouras</a>, 
                <a href="https://scholar.google.com/citations?user=AE5suDoAAAAJ&hl=en" class="author-link">Haitham Bou-Ammar</a>, 
                <a href="http://www0.cs.ucl.ac.uk/staff/jun.wang/" class="author-link">Jun Wang</a>
            </p>
                <p class="affiliations">Huawei Noah's Ark Lab, London, UK | University College London, UK</p>
            <!-- <p class="conference">xxx 2024 (oral/poster)</p>-->
            <iframe width="560" height="315" src="https://www.youtube.com/embed/gWoh_5fsZpA?si=wySuvhbl0TFt-guF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            <nav>
                <a href="https://arxiv.org/pdf/2407.09450" class="button">Paper</a>
                <a href="https://github.com/em-llm/EM-LLM-model" class="button">GitHub</a>
                <a href="https://huggingface.co/papers/2407.09450" class="button">Hugging Face</a>
                <a href="https://x.com/zfountas/status/1812854706051461441" class="button">Twitter</a>
                <!--<a href="#" class="button">Code</a>
                <a href="#" class="button">Video</a>-->
            </nav>
        </div>
        <!--<div class="header-visual">
             Replace with actual path to your key figure or video embed 
            <img src="video.png" alt="EM-LLM Key Figure"> 
        </div>-->
    </header>

    <main>
        <section id="abstract">
            <h2>Summary</h2>
            <p>While typical large language models (LLMs) struggle with processing extensive contexts, the human brain excels at organising and retrieving experiences spanning a lifetime. In this work, we introduce <b>EM-LLM</b>, an architecture that integrates key aspects of human episodic memory and event cognition into LLMs with <b>no fine-tuning</b>, enabling them to handle practically infinite context lengths while maintaining computational efficiency.</p>
            <p>EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an online fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient and human-like access to relevant information.</p>
            <p>Experiments on the <i><a class="dataset-link" href="https://github.com/THUDM/LongBench"><strong>LongBench</strong></a></i> and <I><a class="dataset-link" href="https://github.com/OpenBMB/InfiniteBench"><strong>∞-Bench</strong></a></i> benchmarks demonstrate EM-LLM's superior performance, consistently outperforming the SOTA retrieval model <a class="dataset-link" href="https://arxiv.org/abs/2402.04617"><strong>InfLLM</strong></a> across various baseline LLMs. In addition, EM-LLM outperforms <a class="dataset-link" href="#"><strong>SOTA RAG</strong><a></a> retrieval models in a wide range of tasks, while requiring similar resources. Notably, EM-LLM's performance even surpasses <strong>full-context models</strong> in most tasks, while successfully performing retrieval across <strong>10M tokens</strong> - a scale computationally infeasible for such models. Our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting a bridge between this artificial system and its biological counterpart, thereby offering a novel computational framework for exploring human memory mechanisms.</p>
        </section>

        <section id="architecture">
            <h2>Architecture</h2>
            <p>EM-LLM integrates crucial aspects of event cognition and episodic memory into Transformer-based LLMs. Our approach includes:</p>
            <ul>
                <li>Surprise-based event segmentation</li>
                <li>Graph-theoretic boundary refinement</li>
                <li>Two-stage memory retrieval process</li>
            </ul>
            <img src="Fig_architecture_only.svg" alt="EM-LLM Architecture" class="figure" style="width: 80%;">
        </section>

        <section id="results">
            <h2>Performance Results</h2>
            <p>
                We tested EM-LLM on the <a href="https://github.com/THUDM/LongBench" class="dataset-link"><strong>LongBench</strong></a> and <a href="https://github.com/OpenBMB/InfiniteBench" class="dataset-link"><strong>∞-Bench</strong></a> benchmarks, across a wide range of long-context tasks (including tasks with millions of tokens), comparing it to the current state-of-the-art in both RAG retrievals and other long-context models. Here's a quick look at how we did:
            </p>

            <h3>EM-LLM vs RAG and full-context models</h3>
            <p>
            <div align="center">
              <img src="./Fig_rag_fc_10M.svg" alt="emllm_rag_fc" width="40%"/>
                <p class="table-caption">
                  <strong>Figure:</strong> <strong>(Left)</strong> EM-LLM$_S$ vs. RAG (NV-Embed-v2 retriever) vs. full-context, with LLaMA-3.1-8B as the base LLM, evaluated on LongBench. <strong>(Right)</strong> Comparison of various long-sequence methods (sorted based on their context window length) on an extended version of $\infty$-Bench's _Retrieve.PassKey_.
                </p>
            </div>
            </p>
      
            <h3>EM-LLM vs <a href="#" class="dataset-link">InfLLM</a></h3>
            <div class="table-container">
                <table class="results-table concise">
                    <thead>
                        <tr>
                            <th>Task</th>
                            <th>InfLLM</th>
                            <th>Best EM-LLM (Imp.)</th>
                            <th>Task</th>
                            <th>InfLLM</th>
                            <th>Best EM-LLM (Imp.)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>NarrativeQA</td>
                            <td>22.12</td>
                            <td>22.45 <span class="improvement">(+1.49%)</span></td>
                            <td>SAMSum</td>
                            <td>42.52</td>
                            <td>42.89 <span class="improvement">(+0.87%)</span></td>
                        </tr>
                        <tr>
                            <td>MultiNews</td>
                            <td>26.70</td>
                            <td>26.69 <span class="decline">(-0.04%)</span></td>
                            <td>2WikiMQA</td>
                            <td>22.31</td>
                            <td>23.74 <span class="improvement">(+6.41%)</span></td>
                        </tr>
                        <tr>
                            <td>Qasper</td>
                            <td>29.33</td>
                            <td>29.38 <span class="improvement">(+0.17%)</span></td>
                            <td>PassageRetrieval</td>
                            <td>64.00</td>
                            <td>85.42 <span class="improvement">(+33.47%)</span></td>
                        </tr>
                        <tr>
                            <td>TREC</td>
                            <td>69.00</td>
                            <td>70.50 <span class="improvement">(+2.17%)</span></td>
                            <td>Musique</td>
                            <td>17.68</td>
                            <td>18.77 <span class="improvement">(+6.17%)</span></td>
                        </tr>
                        <tr>
                            <td>MultiFieldQA</td>
                            <td>47.42</td>
                            <td>47.62 <span class="improvement">(+0.42%)</span></td>
                            <td>LCC</td>
                            <td>56.67</td>
                            <td>57.03 <span class="improvement">(+0.63%)</span></td>
                        </tr>
                        <tr>
                            <td>TriviaQA</td>
                            <td>86.67</td>
                            <td>87.62 <span class="improvement">(+1.10%)</span></td>
                            <td>GovReport</td>
                            <td>31.03</td>
                            <td>31.62 <span class="improvement">(+1.90%)</span></td>
                        </tr>
                        <tr>
                            <td>HotpotQA</td>
                            <td>36.56</td>
                            <td>39.99 <span class="improvement">(+9.38%)</span></td>
                            <td>RepoBench-P</td>
                            <td>52.97</td>
                            <td>53.68 <span class="improvement">(+1.34%)</span></td>
                        </tr>
                        <tr>
                            <td>QMSum</td>
                            <td>23.49</td>
                            <td>23.99 <span class="improvement">(+2.13%)</span></td>
                            <td><strong>Average Score:</strong></td>
                            <td><strong>41.90</strong></td>
                            <td><strong>43.70</strong> <span class="improvement">(+4.30%)</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <p class="table-caption">
                <strong>Table 1:</strong> EM-LLM performance on LongBench compared to our baseline InfLLM. 
                The "Best EM-LLM" column shows the highest score achieved by an EM-LLM variant (either with/out boundary refinement and contiguity buffer), with the relative improvement over InfLLM in parentheses.
            </p>

            <div class="table-container">
                <table class="results-table concise">
                    <thead>
                        <tr>
                            <th>Task</th>
                            <th>InfLLM</th>
                            <th>Best EM-LLM (Imp.)</th>
                            <th>Task</th>
                            <th>InfLLM</th>
                            <th>Best EM-LLM (Imp.)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Code debug</td>
                            <td>29.44</td>
                            <td>29.7 <span class="improvement">(+0.9%)</span></td>
                            <td>Passkey</td>
                            <td>100</td>
                            <td>100</td>
                        </tr>
                        <tr>
                            <td>Math find</td>
                            <td>26.57</td>
                            <td>28.0 <span class="improvement">(+5.4%)</span></td>
                            <td>Number string</td>
                            <td>99.83</td>
                            <td>99.83</td>
                        </tr>
                        <tr>
                            <td>KV retrieval</td>
                            <td>95.6</td>
                            <td>99.0 <span class="improvement">(+3.6%)</span></td>
                            <td><strong>Average Score:</strong></td>
                            <td><strong>70.29</strong></td>
                            <td><strong>71.31</strong> <span class="improvement">(+1.45%)</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <p class="table-caption">
                <strong>Table 2:</strong> EM-LLM performance on <a href="#" class="dataset-link"><strong>∞-Bench</strong></a> tasks compared to our baseline InfLLM.
            </p>

            <p>
                As you can see, EM-LLM shows consistent improvements, with standout performances in PassageRetrieval, HotpotQA and KV retrieval tasks.
            </p>

            <p>
                But wait, there's more! We'll be updating this page so stay tuned for these expanded results, which will give an even fuller picture of EM-LLM's capabilities.
            </p>
        </section>

        <section id="human-correlation">
            <h2>Human-like Event Segmentation</h2>
            <p>
                Our analysis reveals strong correlations between EM-LLM's surprise-based event segmentation and human-perceived events, suggesting a bridge between these two systems. 
                For example, consider the figure below:
            </p>
            <img src="Fig_human_res_llama2_2.svg" alt="Human-LLM Correlation in Event Segmentation" class="figure">
            <p>
                These graphs present results from a <a href="https://arxiv.org/abs/2301.10297">study</a> where participants listened to a podcast and indicated points they perceived as event boundaries. 
                We then compared various AI segmentation methods, including EM-LLM, against these human annotations. 
                The height of each bar represents how closely the method aligns with human judgments. 
                Notably, our surprise-based approaches (<b>S</b>, <b>SM</b>, <b>SC</b>) consistently outperform fixed-interval methods (<b>F</b>, <b>FM</b>, <b>FC</b>), with EM-LLM closely mirroring human intuition. 
                This alignment suggests that EM-LLM's event detection mechanism captures something fundamental about how humans naturally segment continuous experiences.
            </p>

        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>EM-LLM represents a significant step forward in the development of language models with extended context-processing capabilities. By bridging insights from cognitive science with machine learning, our approach not only enhances the performance of LLMs on long-context tasks but also provides a scalable computational framework for testing hypotheses about human memory.</p>
        </section>


        <section id="cite">
            <h2>Cite Us</h2>
            <div class="citation-container">
                <pre><code id="citationText">
@misc{fountas2024humanlikeepisodicmemoryinfinite,
      title={Human-like Episodic Memory for Infinite Context LLMs}, 
      author={Zafeirios Fountas and Martin A Benfeghoul and Adnan Oomerjee and Fenia Christopoulou and Gerasimos Lampouras and Haitham Bou-Ammar and Jun Wang},
      year={2024},
      eprint={2407.09450},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.09450}, 
}
                </code></pre>
                <button id="copyButton" aria-label="Copy citation">
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                </button>
            </div>
        </section>
    
   
        <script>
        document.getElementById('copyButton').addEventListener('click', function() {
            var citationText = document.getElementById('citationText').textContent;
            navigator.clipboard.writeText(citationText).then(function() {
                alert('Citation copied to clipboard!');
            }, function(err) {
                console.error('Could not copy text: ', err);
            });
        });
        </script>
    </main>

    <footer>
        <p>&copy; 2024 Huawei Noah's Ark Lab, London, UK</p>
    </footer>
</body>
</html>
